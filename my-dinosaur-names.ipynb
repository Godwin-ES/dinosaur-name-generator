{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":189031,"sourceType":"datasetVersion","datasetId":81348},{"sourceId":2485438,"sourceType":"datasetVersion","datasetId":1504471}],"dockerImageVersionId":30162,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ekanemgodwin/my-dinosaur-names?scriptVersionId=195450787\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Name Generation with Keras RNNs: Learn to Generate Realistic Dinosaur and City Names\n\nThis notebook was inspired by the programming assignment *Dinosaur Island-Character-Level Language Modeling* in the course [\"Sequence Models\" on Coursera](https://www.coursera.org/learn/nlp-sequence-models).","metadata":{}},{"cell_type":"code","source":"# import frequently used libraries\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-30T15:57:08.394321Z","iopub.execute_input":"2024-08-30T15:57:08.394723Z","iopub.status.idle":"2024-08-30T15:57:08.400937Z","shell.execute_reply.started":"2024-08-30T15:57:08.394679Z","shell.execute_reply":"2024-08-30T15:57:08.399737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check for available input files with examples to show to the network during training. Multiple datasets are linked to this notebook as input and all of them will be used to train different models later:","metadata":{}},{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:57:08.403617Z","iopub.execute_input":"2024-08-30T15:57:08.404069Z","iopub.status.idle":"2024-08-30T15:57:08.41988Z","shell.execute_reply.started":"2024-08-30T15:57:08.404005Z","shell.execute_reply":"2024-08-30T15:57:08.418639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dinosaur Name Dataset\n\n## Preparing the dataset\n\nImport the dinosaur name dataset and investigate the data it contains. There are about 1500 dino names, sorted alphabetically. Note that the names all have different length.","metadata":{}},{"cell_type":"code","source":"dino_corpus = pd.read_csv(\"/kaggle/input/dinosaur-island/dinos.txt\", header=None, names=['name'])\ndino_corpus = dino_corpus.drop_duplicates()\n\ndino_corpus","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:57:08.421458Z","iopub.execute_input":"2024-08-30T15:57:08.422183Z","iopub.status.idle":"2024-08-30T15:57:08.44318Z","shell.execute_reply.started":"2024-08-30T15:57:08.422127Z","shell.execute_reply":"2024-08-30T15:57:08.442075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import namedtuple\nVocabulary = namedtuple('Vocabulary', ['VOCAB_SIZE', 'END_TOKEN', 'character2index', 'index2character', 'c_to_i', 'i_to_c'])\n\ndef get_vocabulary(data_column):\n    all_characters = set()\n    for entry in data_column:\n        all_characters |= set(entry.lower()) #shorthand in Python for performing an in-place union of sets.\n    \n    all_characters = sorted(all_characters)\n    \n    print(f\"The vocabulary includes {len(all_characters)} characters: {all_characters}\")\n    \n    char2idx = {}\n    idx2char = {}\n    \n    for idx, char in enumerate(all_characters):\n        char2idx[char] = idx+1\n        idx2char[idx+1] = char\n\n    end_token = 0 \n    \n    char2idx['END'] = end_token\n    idx2char[end_token] = 'END'\n    \n    vocab_size = len(all_characters) + 1\n    \n    # turn into function so it's callable\n    char2idx_fcn = lambda c: char2idx[c]\n    idx2char_fcn = lambda i: idx2char[i]\n    \n    return Vocabulary(vocab_size, end_token, char2idx_fcn, idx2char_fcn, char2idx, idx2char)\n\ndino_vocab = get_vocabulary(dino_corpus['name'])","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:57:08.444585Z","iopub.execute_input":"2024-08-30T15:57:08.444861Z","iopub.status.idle":"2024-08-30T15:57:08.462605Z","shell.execute_reply.started":"2024-08-30T15:57:08.44482Z","shell.execute_reply":"2024-08-30T15:57:08.458462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define some helper functions to convert between words, indices and one-hot representation","metadata":{}},{"cell_type":"code","source":"def onehot2indices(hotmax):\n    return tf.math.argmax(hotmax, axis=-1).numpy().flatten()\n\ndef word2indices(word, vocab):\n    return [vocab.character2index(c.lower()) for c in word]\n\ndef indices2word(indices, vocab):\n    return ''.join([vocab.index2character(i) for i in indices])","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:57:08.465981Z","iopub.execute_input":"2024-08-30T15:57:08.466459Z","iopub.status.idle":"2024-08-30T15:57:08.476234Z","shell.execute_reply.started":"2024-08-30T15:57:08.466404Z","shell.execute_reply":"2024-08-30T15:57:08.475105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TrainingData = namedtuple('TrainingData', ['inputs', 'targets'])\n\ndef convert_to_training_data(raw_data, vocab):\n    # Convert the strings to arrays of indices\n    indices = raw_data.map(lambda w: word2indices(w.strip(), vocab))\n    \n    # Prepend inputs with invalid index (-1), will result in all-zero one-hot encoding\n    inputs = indices.map(lambda l: [-1] + l)\n    # Append outputs with END_TOKEN to signal end of the word\n    outputs = indices.map(lambda l: l + [vocab.END_TOKEN])\n    \n    input_arrays = list(inputs)\n    output_arrays = list(outputs)\n\n    # Determine maximum sequence length\n    max_length = max(max(len(seq) for seq in input_arrays), max(len(seq) for seq in output_arrays))\n    \n    # Pad sequences to ensure uniform length\n    X_train = pad_sequences(input_arrays, maxlen=max_length, padding='post', value=vocab.VOCAB_SIZE)\n    y_train = pad_sequences(output_arrays, maxlen=max_length, padding='post', value=vocab.VOCAB_SIZE)\n    \n    # Convert to one-hot encoding\n    X_train = tf.one_hot(X_train, depth=vocab.VOCAB_SIZE)\n    y_train = tf.one_hot(y_train, depth=vocab.VOCAB_SIZE)\n    \n    return TrainingData(X_train, y_train)\n\ndino_training_data = convert_to_training_data(dino_corpus['name'], dino_vocab)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:57:08.477696Z","iopub.execute_input":"2024-08-30T15:57:08.478402Z","iopub.status.idle":"2024-08-30T15:57:08.547357Z","shell.execute_reply.started":"2024-08-30T15:57:08.478358Z","shell.execute_reply":"2024-08-30T15:57:08.54635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RNN Model Architecture\n\nStandard parameters","metadata":{}},{"cell_type":"code","source":"NameGenerationModel = namedtuple('NameGenerationModel', ['training_model', 'sampling_cell'])\n\nBATCH_SIZE = 64\nNUM_EPOCHS = 100\n\n# Choice of optimizer - Adam and RMSprop both seem to work well\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n\nlr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, verbose=1, patience=10, cooldown=30)\n\ncosine_similarity = tf.keras.metrics.CosineSimilarity(axis=1)\n\n# Uncomment this for better debugging of the custom metric\n# tf.config.run_functions_eagerly(True)\n\n@tf.function\ndef character_acc(y_true, y_pred, axis=1):\n    num_characters = tf.cast(tf.size(y_true) / tf.shape(y_true)[axis], tf.float32)\n    idx_true = tf.math.argmax(y_true, axis=axis)\n    idx_pred = tf.math.argmax(y_pred, axis=axis)\n    matching_count = tf.cast(tf.math.count_nonzero(idx_true == idx_pred), tf.float32)\n    score = matching_count / num_characters\n    return score\n\ndef plot_history(history):\n    for key, values in history.items():\n        plt.plot(values)\n        plt.title(key)\n        plt.xlabel('epoch')\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:57:08.549051Z","iopub.execute_input":"2024-08-30T15:57:08.54952Z","iopub.status.idle":"2024-08-30T15:57:08.568033Z","shell.execute_reply.started":"2024-08-30T15:57:08.549465Z","shell.execute_reply":"2024-08-30T15:57:08.566952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For generating new names after training\n- `generate_name1` samples next character from the probability distribution\n- `generate_name2` incorporates temperature scaling","metadata":{}},{"cell_type":"code","source":"def generate_name1(model, char_to_idx, idx_to_char, max_length=20):\n    # Initialize the sequence with zeros\n    input_seq = np.zeros((1, 1, len(char_to_idx)))\n\n    # List to store generated characters\n    generated_name = []\n\n    for _ in range(max_length):\n        # Predict the next character\n        predictions = model.predict(input_seq, verbose=0)\n        predictions = predictions[0, -1, :]\n        next_idx = np.random.choice(range(len(predictions)), p = predictions)\n\n        # Convert the predicted index to a character\n        next_char = idx_to_char[next_idx]\n        if next_char == 'END':\n            break\n            \n        generated_name.append(next_char)\n\n        # Update the input sequence with the new character\n        new_input_seq = np.zeros((1, 1, len(char_to_idx)))\n        new_input_seq[0, 0, next_idx] = 1\n        input_seq = np.concatenate([input_seq, new_input_seq], axis=1)\n\n    # Convert list of characters to a string\n    return ''.join(generated_name)\n\ndef sample(predictions, temperature=1.0):\n    \"\"\"\n    Sample from the probability distribution.\n    \"\"\"\n    predictions = np.asarray(predictions).astype('float64')\n    #take log of probabilities (add 1e-7 to prevent log of 0), then use temperature scaling\n    #lower temp means more confident predictions, higher temp is more random\n    predictions = np.log(predictions + 1e-7) / temperature \n    exp_preds = np.exp(predictions)\n    preds = exp_preds / np.sum(exp_preds)\n    return np.random.choice(len(predictions), p=preds)\n\ndef generate_name2(model, char_to_idx, idx_to_char, max_length=20, temperature=1.0):\n    # Initialize the sequence with zeros\n    input_seq = np.zeros((1, 1, len(char_to_idx)))\n\n    # List to store generated characters\n    generated_name = []\n\n    for _ in range(max_length):\n        # Predict the next character\n        predictions = model.predict(input_seq, verbose=0)[0, -1, :]\n        \n        # Sample from the predictions\n        next_idx = sample(predictions, temperature)\n\n        # Convert the predicted index to a character\n        next_char = idx_to_char[next_idx]\n        # Stop if the end token is predicted\n        if next_char == 'END':\n            break\n            \n        generated_name.append(next_char)\n\n        # Update the input sequence with the new character\n        new_input_seq = np.zeros((1, 1, len(char_to_idx)))\n        new_input_seq[0, 0, next_idx] = 1\n        input_seq = np.concatenate([input_seq, new_input_seq], axis=1)\n\n    # Convert list of characters to a string\n    return ''.join(generated_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:57:08.571001Z","iopub.execute_input":"2024-08-30T15:57:08.571334Z","iopub.status.idle":"2024-08-30T15:57:08.594363Z","shell.execute_reply.started":"2024-08-30T15:57:08.571294Z","shell.execute_reply":"2024-08-30T15:57:08.593108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## My model","metadata":{}},{"cell_type":"code","source":"def get_model2(memory_size, vocab_size):\n    model = tf.keras.Sequential([\n        tf.keras.Input((None, vocab_size)),  # Input layer for sequences of one-hot encoded vectors\n        tf.keras.layers.Masking(mask_value=0), #ignore padding values\n        tf.keras.layers.SimpleRNN(memory_size, return_sequences=True, activation='tanh'),  # SimpleRNN layer\n        tf.keras.layers.Dense(vocab_size, activation='softmax')  # Dense layer to output probabilities\n    ])\n    model.summary()\n    \n    return NameGenerationModel(model, None)  # `None` for sampling_cell since it's not needed\n\n# Define the memory size and vocabulary size\nMEMORY_SIZE = 50\ndino_model2 = get_model2(MEMORY_SIZE, dino_vocab.VOCAB_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:57:08.595957Z","iopub.execute_input":"2024-08-30T15:57:08.596339Z","iopub.status.idle":"2024-08-30T15:57:08.730144Z","shell.execute_reply.started":"2024-08-30T15:57:08.596292Z","shell.execute_reply":"2024-08-30T15:57:08.728759Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dino_model2.training_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[character_acc, cosine_similarity])\n\ndino_history2 = dino_model2.training_model.fit(x=dino_training_data.inputs, y=dino_training_data.targets, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, callbacks=[lr_callback])","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:57:08.732019Z","iopub.execute_input":"2024-08-30T15:57:08.732864Z","iopub.status.idle":"2024-08-30T15:58:28.367237Z","shell.execute_reply.started":"2024-08-30T15:57:08.732807Z","shell.execute_reply":"2024-08-30T15:58:28.366048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_history(dino_history2.history)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:58:28.369383Z","iopub.execute_input":"2024-08-30T15:58:28.370149Z","iopub.status.idle":"2024-08-30T15:58:29.036092Z","shell.execute_reply.started":"2024-08-30T15:58:28.370091Z","shell.execute_reply":"2024-08-30T15:58:29.035154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage:\ngenerated_name = generate_name1(dino_model2.training_model, dino_vocab.c_to_i, dino_vocab.i_to_c)\nprint(\"Generated Dinosaur Name:\", generated_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:58:29.037872Z","iopub.execute_input":"2024-08-30T15:58:29.03843Z","iopub.status.idle":"2024-08-30T15:58:31.463005Z","shell.execute_reply.started":"2024-08-30T15:58:29.038379Z","shell.execute_reply":"2024-08-30T15:58:31.461923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage with default temperature:\ngenerated_name = generate_name2(dino_model2.training_model, dino_vocab.c_to_i, dino_vocab.i_to_c)\nprint(\"Generated Dinosaur Name with Temperature 1.0:\", generated_name)\n\n# Example usage with temperature adjustment:\ngenerated_name = generate_name2(dino_model2.training_model, dino_vocab.c_to_i, dino_vocab.i_to_c, temperature=0.7)\nprint(\"Generated Dinosaur Name with Temperature 0.7:\", generated_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:58:31.464543Z","iopub.execute_input":"2024-08-30T15:58:31.464875Z","iopub.status.idle":"2024-08-30T15:58:33.338838Z","shell.execute_reply.started":"2024-08-30T15:58:31.464827Z","shell.execute_reply":"2024-08-30T15:58:33.337658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dino_model2.training_model.save('dino_model.keras')","metadata":{"execution":{"iopub.status.busy":"2024-08-30T15:58:57.377402Z","iopub.execute_input":"2024-08-30T15:58:57.377766Z","iopub.status.idle":"2024-08-30T15:58:57.412774Z","shell.execute_reply.started":"2024-08-30T15:58:57.37772Z","shell.execute_reply":"2024-08-30T15:58:57.411684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model = tf.keras.models.load_model(\n    \"/kaggle/working/dino_model.keras\",\n    custom_objects={'character_acc': character_acc}\n)\n\ngenerated_name = generate_name2(loaded_model, dino_vocab.c_to_i, dino_vocab.i_to_c, temperature=0.7)\nprint(\"Generated Dinosaur Name:\", generated_name)","metadata":{"execution":{"iopub.status.busy":"2024-08-30T16:10:50.678779Z","iopub.execute_input":"2024-08-30T16:10:50.67911Z","iopub.status.idle":"2024-08-30T16:10:52.488843Z","shell.execute_reply.started":"2024-08-30T16:10:50.679072Z","shell.execute_reply":"2024-08-30T16:10:52.487915Z"},"trusted":true},"execution_count":null,"outputs":[]}]}